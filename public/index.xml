<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>while true: continue</title>
    <link>http://jad-b.github.io/</link>
    <description>Recent content on while true: continue</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Jul 2016 10:43:26 -0400</lastBuildDate>
    <atom:link href="http://jad-b.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Concept for a Vice Driven Deployment System</title>
      <link>http://jad-b.github.io/post/Concept%20for%20a%20Vice-Driven%20Deployment%20System/</link>
      <pubDate>Fri, 29 Jul 2016 10:43:26 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Concept%20for%20a%20Vice-Driven%20Deployment%20System/</guid>
      <description>&lt;p&gt;Alright, here me out: we could totally write a deployment service that was leveraged each of the seven cardinal sins.
Here&amp;rsquo;s my take on it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pride - Deploys code&lt;/li&gt;
&lt;li&gt;Envy - Manages config&lt;/li&gt;
&lt;li&gt;Wrath - Runs functional/smoke tests&lt;/li&gt;
&lt;li&gt;Gluttony - Data visualization; &amp;ldquo;feast your eyes&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Lust - Messaging &amp;amp; notifications; something about sharing with other people,
so the community aspect&lt;/li&gt;
&lt;li&gt;Sloth - Deployment automation; pride =&amp;gt; sloth&lt;/li&gt;
&lt;li&gt;Greed - Deployment data; greed would feed gluttony&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throw this behind a CLI &amp;amp; chat bot, and ta-da, you&amp;rsquo;re a &lt;code&gt;vice pride deploy
&amp;lt;project&amp;gt;&lt;/code&gt; away from releasing code.&lt;/p&gt;

&lt;p&gt;Now, I&amp;rsquo;m going to be partial to this because I&amp;rsquo;m in love with the cleverness of
my own ideas.  You&amp;rsquo;d have to make it good, and you&amp;rsquo;d have to make it
future-proof. This is how I&amp;rsquo;d do it:&lt;/p&gt;

&lt;p&gt;Write the interface first. If you&amp;rsquo;ve mastered the deploy of &lt;em&gt;any&lt;/em&gt; piece of
reasonably complex software (a.k.a. greater than just &lt;code&gt;scp&lt;/code&gt;-ing files to a
box), you know what you want from a deployment assistant. Code up that
interface, and stub out the functionality as you go. I&amp;rsquo;d want something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Retrieve configuration for the deploy; could subsume within the next step
vice envy &amp;lt;options...&amp;gt; &amp;lt;project&amp;gt;
# Deploy our project!
vice pride deploy &amp;lt;project&amp;gt;
# Run our post-release smoke tests
vice wrath &amp;lt;project&amp;gt; &amp;lt;options...&amp;gt;
# Visualize how our deploy went
vice gluttony report &amp;lt;project&amp;gt;
# See how _all_ our projects are looking
vice gluttony report &amp;lt;saved dashboard name&amp;gt;
# Drop a message in Slack about the release
vice lust project -m &amp;quot;Everything&#39;s looking good, guys&amp;quot;
# Backup our deploy data into S3
vice greed backup &amp;lt;options...&amp;gt; &amp;lt;S3 URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which means I&amp;rsquo;d need a way of making &lt;code&gt;vice&lt;/code&gt; (I guess the collective name of
this tool is &lt;code&gt;vice&lt;/code&gt;? Is now.) aware of &lt;code&gt;&amp;lt;project&amp;gt;&lt;/code&gt;. Communication is either
push- or pull-based, so it&amp;rsquo;s either that &lt;code&gt;vice&lt;/code&gt; know&amp;rsquo;s how to find projects, or
projects register themselves with &lt;code&gt;vice&lt;/code&gt;. But, you know what? &lt;em&gt;That&lt;/em&gt; matters
far less than what I want my interaction with &lt;code&gt;vice&lt;/code&gt; to be like. Both models
would work, just like Chef and Ansible work. If one&amp;rsquo;s better than the other,
and we start with the lesser, it&amp;rsquo;ll be our good engineering practices of
writing loosely-coupled, modular code that saves us headaches during the refactor.&lt;/p&gt;

&lt;p&gt;Which brings me to modularity. If you want code to survive over time, it has to
be modular, which in this context means we need to be able to write new
backends for emerging technologies that satisfy the same functional interfaces.
Was that a mouthful? Try this: We want &lt;code&gt;vice&lt;/code&gt; to be compatible with new tech.
Not in an auto-magical &amp;ldquo;&lt;code&gt;vice&lt;/code&gt; will work with &lt;em&gt;any database ever&lt;/em&gt;&amp;rdquo; way, but
rather we know what data storage requirements we&amp;rsquo;ll have for &lt;code&gt;greed&lt;/code&gt;, which
means we can write adapters to hide whether we&amp;rsquo;re storing it in Postgresql,
DynamoDB, a file system, or HDFS.&lt;/p&gt;

&lt;p&gt;Maybe we won&amp;rsquo;t be able to completely satisfy our DataStorer interface with each
backend - that&amp;rsquo;s okay! Do a &lt;code&gt;raise NotImplemented&lt;/code&gt;, and allow your calling code
to deal with partial interfaces in non-critical functionality. Obviously you
need to read &amp;amp; write data, but maybe you can&amp;rsquo;t provide the same level of
searching you&amp;rsquo;d like across all backends. Okay. Life is a moving target. Choose
your trade-offs, and above all stay flexible through modular code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-[_blank_] Modeling</title>
      <link>http://jad-b.github.io/post/Multi-_%20Modeling/</link>
      <pubDate>Thu, 28 Jul 2016 10:26:22 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Multi-_%20Modeling/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Multilevel models are statistical models of parameters that vary at more than one level. - source: [Multilevel model:wikipedia].&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This implies that the same parameters can take different values amongst sibling models of different parents. To
illustrate, let&amp;rsquo;s work through the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multilevel_model#Example&#34;&gt;example given in wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;A basic linear regression model for predicting income may take age, class, gender, and race as parameters:
&lt;code&gt;$f(age,class,gender,race) = income$&lt;/code&gt;. However, this is likely to predict the same income for a 45-year old black man in
Seattle, WA as it would for one in Mobile, AL. A simple way to attempt account for this would be to determine the &lt;em&gt;bias&lt;/em&gt;
for each location, and add or subtract that from the estimated income. However, it&amp;rsquo;s obvious this is pretty naive; it
attempts to account for &lt;em&gt;all&lt;/em&gt; the differences between locations by a fixed value.&lt;/p&gt;

&lt;p&gt;A multilevel model allows our parameters of age, class, gender, and race, to take different values per location, by
adding location as a level to our model. Now, our income estimators are scoped by location.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s still assumed that &lt;em&gt;all&lt;/em&gt; values for each parameters are drawn from a shared underlying distribution. To explain,
consider all the values we&amp;rsquo;d likely learn to explain the impact of age, a.k.a the coefficient for the age parameter,
a.k.a &lt;code&gt;$\theta_{age}Age$&lt;/code&gt;. Let&amp;rsquo;s pretend that the bigger the location, the more age benefits you; population will be
proportional to the value with which we weight age, &lt;code&gt;$\theta_{age}Age$&lt;/code&gt;. As there are more people in urban areas,
the distribution of age coefficients will be weighted towards the higher end to reflect that
&lt;em&gt;in general&lt;/em&gt;, people make more as they get older.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll posit that people make more as they get older. We could even pretend it&amp;rsquo;s a linear relationship, as simple as
&lt;code&gt;$income = poverty_line + \theta_{age}Age$&lt;/code&gt;. Now, what if we observe that people in cities make &lt;em&gt;even more&lt;/em&gt; as they get
older than people who aren&amp;rsquo;t? And, it turns out, that a majority of older (50+) people live in cities? Well, that means
the values we&amp;rsquo;re calculating to weight the impact of age, a.k.a the age coefficients, a.k.a &lt;code&gt;$\theta_{age}$&lt;/code&gt;,
will reward age more heavily than if the majority of older people &lt;em&gt;weren&amp;rsquo;t&lt;/em&gt; in cities. So the distribution of &lt;em&gt;all&lt;/em&gt;
the age coefficients, &lt;code&gt;$\theta_{age}$&lt;/code&gt; we&amp;rsquo;ll learn per location will favor age, up to a point.&lt;/p&gt;

&lt;p&gt;Multilevel models are subclasses of Hierarchical Bayesian Models.&lt;/p&gt;

&lt;p&gt;Select between models using Bayesian|Akakike Information Criteria (the equivalent of &lt;code&gt;score()&lt;/code&gt; in sklearn.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lessons Learned: Software</title>
      <link>http://jad-b.github.io/post/Lessons%20Learned:%20Software/</link>
      <pubDate>Thu, 28 Jul 2016 09:59:13 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Lessons%20Learned:%20Software/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Use TLS from the beginning.&lt;/strong&gt;
There aren&amp;rsquo;t many reasons you shouldn&amp;rsquo;t. Snakeoil gets you in the practice of
managing the keys, but a self-signed or corporate wildcard cert that&amp;rsquo;s scoped
to your development domain are even better. I&amp;rsquo;m finally seeing a plethora of
key management tools&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:kmt&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:kmt&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; begin to propagate, so no excuses.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Write functional tests to run against any envinromnet.&lt;/strong&gt;
This means they need to accept configuration options, and should only
affect test data they&amp;rsquo;re responsible for creating and deleting. Now your
integration tests can smoke-test production.
If you can scope which tests run, which I know Python&amp;rsquo;s &lt;code&gt;nose&lt;/code&gt; and &lt;code&gt;pytest&lt;/code&gt;
and Golang&amp;rsquo;s &lt;code&gt;go test -tags=smoke&lt;/code&gt; can do, you can limit your tests to fit
your requirements of run time, data safety, and validation.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Incremental modifications over re-writes.&lt;/strong&gt;
You have code running, and now you need to change it. It&amp;rsquo;s conceptually
easiest to pretend like that code doesn&amp;rsquo;t exist, and write from scratch.
But only in concept. Re-writes ignore the ecosystem that code runs within -
the system&amp;rsquo;s that depend on it, the operational considerations you&amp;rsquo;ve learned to
account for, and the realities of the world you&amp;rsquo;ve had to code around.
Re-writes have a place. But they&amp;rsquo;re usually the harder choice, long-term.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s some ideas I&amp;rsquo;ve already had to use:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Don&amp;rsquo;t forget about search &amp;amp; replace. Rename old objects to
&amp;ldquo;OldStructure&amp;rdquo;, or &amp;ldquo;StructureV1&amp;rdquo; while the deprecation is underway.
People seem to get nervous about this, but if you missed something, a
static language won&amp;rsquo;t compile, and a dynamically-typed language will have
its tests fail. If both of those fail to catch the error, is it really a
problem with the replacement, or with your safety nets?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;REST API: Isolate the old and new logic into different functions. Deploy
new changes to a &amp;ldquo;dark&amp;rdquo; endoint under a different path. Toggle between
old &amp;amp; new using a query parameter, or swap them at runtime using a flag.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Database: Adding new columns is easy. Modifying existing requires
staggered deploys between the DB and its clients (probably your
downstream API(s)). Sorry.  At some point, you will be faced with the
choice of doing a total re-write, followed by a nerve-wracking deploy,
replete with cross-team coordination, or biting off that complexity in
small changes and frequent deployments. Small changes let you deploy
faster. Deploying faster =&amp;gt; Your work affects people faster. Mattering in
other people&amp;rsquo;s lives =&amp;gt; putting off existential meaningless for one more
day. Search &amp;ldquo;zero-downtime DB migration&amp;rdquo; for plenty of resources. I&amp;rsquo;m
partial to the outline in Chapter 12 of &amp;ldquo;Continuous Delivery&amp;rdquo; by Jez
Humble, titled &amp;ldquo;Managing Data&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Write backwards from your user interfaces&lt;/strong&gt;.
Writing a web app? Start with the UI. You&amp;rsquo;re writing the REST API the UI calls?
Start with the API description. &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification&#34;&gt;OpenAPI is a thing now&lt;/a&gt;, and I&amp;rsquo;m not going
back. It&amp;rsquo;s a CLI tool? Write the options w/ flags, for what you know you
need.&lt;/p&gt;

&lt;p&gt;Only write for what someone needs right now. What someone wants but not needs
should live in a design conversation elsewhere, such as a GitHub issue, a
text document, or a JIRA ticket.&lt;/p&gt;

&lt;p&gt;I believe this to be a (correct) generalization of &amp;ldquo;API-first&amp;rdquo; design and/or
contract-driven design.&lt;/p&gt;

&lt;p&gt;One possible argument: This delays potentially deal-breaking backend
considerations that are far-removed from the end-user experience.&lt;/p&gt;

&lt;p&gt;I think two things are imporant to keep in mind. First, your problem has
likely been solved before, many times over. This is the case for most
full-stack work. Thus, the amount of &amp;ldquo;unknowns&amp;rdquo; lurking out there
are few, and always getting smaller. Second, all this really means is you
have a conversation with your clients (The web UI team) exactly when you were
actually building the tool they wanted. If anyone has examples of a tool
simply can&amp;rsquo;t be made human-friendlier, I&amp;rsquo;d be interested to know.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:kmt&#34;&gt;&lt;p&gt;Such as these:
&lt;a href=&#34;https://aws.amazon.com/kms/&#34;&gt;Amazon KMS&lt;/a&gt;,
&lt;a href=&#34;https://www.vaultproject.io/docs/secrets/pki/index.html&#34;&gt;Hashicorp&amp;rsquo;s Vault&amp;rsquo;s PKI Backend&lt;/a&gt;,
&lt;a href=&#34;https://github.com/square/certstrap&#34;&gt;Square&amp;rsquo;s certstrap&lt;/a&gt;&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:kmt&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why I&#39;m Not Getting My Graduate Degree (Yet).</title>
      <link>http://jad-b.github.io/post/Why%20Not%20School/</link>
      <pubDate>Sat, 28 May 2016 07:42:13 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Why%20Not%20School/</guid>
      <description>&lt;p&gt;In case you &lt;em&gt;haven&amp;rsquo;t&lt;/em&gt; heard, data science is big right now. And when you say
&amp;ldquo;data science&amp;rdquo; you also get &amp;ldquo;machine learning&amp;rdquo; packaged in for free, and that stuff&amp;rsquo;s
cool, so even better!  Plus, the booming field of data engineering is making
all &lt;em&gt;kinds&lt;/em&gt; of advances in dealing with Big Data, like real-time streaming from
massive amounts of data sources, and you can&amp;rsquo;t tell me that&amp;rsquo;s not going to be
necessity for the widgets of tomorrow.&lt;/p&gt;

&lt;p&gt;Maybe, like me, you&amp;rsquo;d like to learn more about all of this. And perhaps,
like me, you&amp;rsquo;ve got a job, and maybe it&amp;rsquo;s a pretty good job. But the knee-jerk
response you hear is, &amp;ldquo;Get your master&amp;rsquo;s degree&amp;rdquo;, or even further, a Ph.D. And
this is a problem. Not just the money&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, or the potential move, or the
commitment, but the lack of &lt;em&gt;optimality&lt;/em&gt;: is this the best way to learn?&lt;/p&gt;

&lt;p&gt;How much of your undergraduate education got left in a notebook, or existed
only for a test? Do you remember anything you didn&amp;rsquo;t actually apply? Did you
ever sit bored in class? How about completely lost (at least a more useful
feeling)? What a waste. You, or someone, &lt;em&gt;paid&lt;/em&gt; for that lost time.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at what a modern &lt;a href=&#34;https://www.google.com/webhp?sourceid=chrome-instant&amp;amp;ion=1&amp;amp;espv=2&amp;amp;ie=UTF-8#q=define%20auto%20didact&#34;&gt;autodidact&lt;/a&gt; can utilize when learning data
science:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Take advantage of a broad selection of online courses to gain initial
exposure to concepts and an beginner to intermediate understanding.&lt;/li&gt;
&lt;li&gt;Anyone can buy textbooks, which are useful twice: once for their table of
contents to tell you what the field looks like, and again for when you need
the details.&lt;/li&gt;
&lt;li&gt;Access to essentially all academic literature, if and when that necessity arises.&lt;/li&gt;
&lt;li&gt;Take advantage of a &lt;em&gt;huge&lt;/em&gt; amount of open-source software.&lt;/li&gt;
&lt;li&gt;Get experience from Kaggle competitions, which have directly led to numerous
people getting jobs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This may seem like a knock against all higher education. It is not. If your
goal is to research, or simply learn all you can about a topic, then I think
academia is the perfect, and perhaps only, option. And there are many things
going for higher-education: Direct access to experts and
prohibitively-expensive equipment, a guaranteed path, a like-minded community,
networking and institutional reputation, and job placement services at the end.
I believe each of those points has a self-learning counter-part, and that
comparison would deserve its own write-up, in the interest of keeping this
focused.&lt;/p&gt;

&lt;p&gt;Ask yourself this: After a year of self-tutelage, would you be &lt;em&gt;worse&lt;/em&gt; off if
you &lt;em&gt;then&lt;/em&gt; decided to start a graduate program? Or, as I believe, would you
find yourself a better student, questions in mind, with answers to satisfy?
The self-generated portfolio from your year of self-teaching would certainly
assist in admissions.&lt;/p&gt;

&lt;p&gt;And, of course, you might find you don&amp;rsquo;t need that graduate&amp;rsquo;s program at all,
or that your interests have changed as a result of what you learned yourself.
No one knows you better than yourself. It won&amp;rsquo;t be easy. You&amp;rsquo;ll have to learn
how &lt;em&gt;you&lt;/em&gt; learn: how to tell when you&amp;rsquo;ve dead-ended, when you&amp;rsquo;re coasting.
You&amp;rsquo;ll have to find your own projects. And you&amp;rsquo;ll have to manage your own
motivation, since you won&amp;rsquo;t have a course progression or grades to praise you.
These are hard things to do. They are also worthwhile things to do. You get to
take them with you the rest of your life.&lt;/p&gt;

&lt;p&gt;I am taking this path. It&amp;rsquo;s already been difficult. Progress feels like drawing
loops without lifting the pen from the page; you go forwards, then sideways,
then backwards, but you never end up where you started, and that becomes your
new starting point. I want to find more people embarking on this new form of
open, project-based, crowd-sourced, self-directed education, no matter their
field of interest. Hopefully our loops will cross paths.&lt;/p&gt;

&lt;p&gt;I write this more for myself than anyone, but don&amp;rsquo;t we always?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;Because average annual tuition costs are $30-40k, and even with
50% financial aid, this is still tens of thousands of dollars. And you need
health insurance. And if you want to get it done fast, you&amp;rsquo;re almost certainly
not working, although a 2-year degree &lt;em&gt;is&lt;/em&gt; possible if you commit the rest of
your non-working hours to school, with an associated hit to your quality of
life.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ODSC 2016 - Friday</title>
      <link>http://jad-b.github.io/post/ODSC2016%20-%20Friday/</link>
      <pubDate>Fri, 20 May 2016 10:21:22 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/ODSC2016%20-%20Friday/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Goto&lt;/strong&gt;: &lt;a href=&#34;No page found with path or logical name &#34;post/ODSC2016 - Saturday.md&#34;.
&#34;&gt;Saturday&lt;/a&gt; &amp;amp;
&lt;a href=&#34;No page found with path or logical name &#34;post/ODSC2016 - Sunday.md&#34;.
&#34;&gt;Sunday&amp;rsquo;s&lt;/a&gt; notes.&lt;/p&gt;

&lt;h1 id=&#34;friday-morning&#34;&gt;Friday - Morning&lt;/h1&gt;

&lt;h2 id=&#34;building-a-recommendation-system&#34;&gt;Building a Recommendation System&lt;/h2&gt;

&lt;p&gt;Speaker: &lt;a href=&#34;https://www.linkedin.com/in/cfregly&#34;&gt;Chris Fregly&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;spark&#34;&gt;Spark&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;On Spark &amp;amp; Tableau (or Redshift):&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you&amp;rsquo;re connecting Tableau to Spark, it&amp;rsquo;s all going through one JVM, the HiveThrift
server (which converts SQL into Java talk).  You want to make sure you&amp;rsquo;re pushding down as
much computation as possible to Spark, saving the HiveThrift server cycles. This way you
collect the results of your computation for display in your visualization engine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;On Graph Analysis&lt;/strong&gt;
If you&amp;rsquo;re doing network analysis using Spark, look at using GraphFrames over GraphX.
GraphFrames is fully supported in Spark2. Both are only good for offline graph analytics;
transactional graph queries should be loaded into a dedicated Graph DB like Neo4j.&lt;/p&gt;

&lt;p&gt;People don&amp;rsquo;t seem to test these systems! I suppose if you&amp;rsquo;re already monitoring
performance and model prediction accuracy, you &lt;em&gt;are&lt;/em&gt; testing them. And it&amp;rsquo;d be easy enough
to mock up I/O for subsets to confirm functionality.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Spark - Packages&lt;/code&gt;: a marketplace for Spark packages&lt;/p&gt;

&lt;h4 id=&#34;spark-2-0&#34;&gt;Spark 2.0&lt;/h4&gt;

&lt;p&gt;Big focus on codegen: A &lt;code&gt;map&lt;/code&gt; followed by a &lt;code&gt;filter&lt;/code&gt; will get re-written into a single method, reducing function calls.
Relies on &lt;a href=&#34;http://unkrig.de/w/Janino&#34;&gt;Project Janino&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lot of talk about
&lt;a href=&#34;https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html&#34;&gt;Tungsten&lt;/a&gt;,
to include new low-level data structures, code generation, and more.&lt;/p&gt;

&lt;p&gt;Spark 2.0 will support exporting models as PMML. PMML is the common data representation of ML models.&lt;/p&gt;

&lt;h4 id=&#34;probabilistic-data-structures&#34;&gt;Probabilistic data structures&lt;/h4&gt;

&lt;p&gt;Paco Nathan: Good work on probabilistic structures. &lt;a href=&#34;https://www.linkedin.com/in/ceteri&#34;&gt;https://www.linkedin.com/in/ceteri&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When you allow the possibility of error into your queries, a 14 bit structure can store
1e9 counts, w/ an error of .81%,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HyperLogLog&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Particular hash function used by HLL guarantees the data will be uniformly distributed.
Something about taking subsets of the data.&lt;/li&gt;
&lt;li&gt;If you have guaranteed uniform distribution, you can check for a distinct number of
users by only checking the beginning of the structure. Similar to Numenta&amp;rsquo;s SDR
bitmasks.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CountMin Sketch&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Creates a (# of hash functions, # of bits) table. Every item gets an entry in each
row (per hash function). Each table cell is a count of how many times a hash
function has created that bit value. When you want the count for an item, you hash
it, and take the &lt;em&gt;minimum&lt;/em&gt; of all rows. You&amp;rsquo;re guaranteed to always be &amp;gt;= the &lt;em&gt;true&lt;/em&gt;
count of the item. The &lt;code&gt;&amp;gt;&lt;/code&gt; is due to overlap with other items.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Also: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;&lt;strong&gt;Bloom Filters&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Size in Memory of 1e6 item

&lt;ul&gt;
&lt;li&gt;HyperLogLog: 16,472 bytes&lt;/li&gt;
&lt;li&gt;Naive Array: 4,800,016 bytes&lt;/li&gt;
&lt;li&gt;CountMin Sketch: 310,944 bytes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Came across this &lt;a href=&#34;https://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;overview&lt;/a&gt; when looking up
info on HyperLogLog.&lt;/p&gt;

&lt;h3 id=&#34;lessons-from-netflix&#34;&gt;Lessons from Netflix&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;A logging company that also streams movies&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Chris worked on the Netflix Streaming Services team as a Data Engineer, and his experience
there came up in many examples.&lt;/p&gt;

&lt;p&gt;Stated &amp;ldquo;Netflix tends to build over buy&amp;rdquo;; if you take their &lt;a href=&#34;https://github.com/netflix&#34;&gt;GitHub
repo&lt;/a&gt; organization as a good representation of what they&amp;rsquo;ve
built, that&amp;rsquo;s 111 repos worth of projects (as of 2016May23, using &lt;code&gt;curl&lt;/code&gt; + &lt;code&gt;jq&lt;/code&gt;). Of
course, they&amp;rsquo;ve already severly limited how much they have to build by heavily leveraging
the capabilities of AWS.&lt;/p&gt;

&lt;p&gt;Noted they&amp;rsquo;ve been using &lt;a href=&#34;https://prestodb.io/&#34;&gt;Presto&lt;/a&gt;, a ANSI SQL tool for querying
multiple datasources at once, for ad-hoc analytics. Presto replaces Apache Hive, as long
as you don&amp;rsquo;t need fault tolerance, as it stores its intermediate results in memory.&lt;/p&gt;

&lt;p&gt;As the number of microservices grew, breakages in one API tended to bring down a &lt;em&gt;lot&lt;/em&gt; of
services. Interestingly, &lt;a href=&#34;https://www.linkedin.com/in/adriancockcroft&#34;&gt;Adrian Cockcroft&lt;/a&gt; mentioned these breakages
tend to show one-level away from the root of the problem in the &lt;em&gt;dependent&lt;/em&gt; APIs, which is
echoes a lesson learned in Athletic Training: Pain starts up the chain. To combat these
breakages, they developed &lt;a href=&#34;https://www.linkedin.com/in/adriancockcroft&#34;&gt;Hystrix&lt;/a&gt;.
Hystrix implements the &lt;a href=&#34;http://martinfowler.com/bliki/CircuitBreaker.html&#34;&gt;Circuit Breaker
pattern&lt;/a&gt;. I haven&amp;rsquo;t dived into the
library well enough to know exactly, but I know it solved a few problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What broke?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s the fallback?&lt;/li&gt;
&lt;li&gt;Metric gathering on API requests.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also: &lt;code&gt;Hystrix&lt;/code&gt; can collapse multiple requests for the same object into one. Natural if
you&amp;rsquo;ve got a connector library going in between all of your API requests.&lt;/p&gt;

&lt;p&gt;Has Hystrix been re-implemented in any other languages?&lt;/p&gt;

&lt;h4 id=&#34;data-pipeline&#34;&gt;Data Pipeline&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;http://www.slideshare.net/cfregly/dc-spark-users-group-march-15-2016-spark-and-netflix-recommendations&#34;&gt;Slideshare here, goto slide 104&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;v1 - Producers =&amp;gt; Chukwa =&amp;gt; S3 =&amp;gt; EMR&lt;/li&gt;
&lt;li&gt;v2 - Producers =&amp;gt; Chukwa {=&amp;gt; S3 =&amp;gt; EMR} &amp;amp; {=&amp;gt; Kafka =&amp;gt; Stream Consumers &amp;amp; more}&lt;/li&gt;
&lt;li&gt;v3 - Producers =&amp;gt; Kafka =&amp;gt; Router =&amp;gt; {S3, ElasticSearch, {Kafka =&amp;gt; Stream Consumers}}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Netflix runs a 10k node Memcached cluster that ML models get loaded into, amongst apparently everything else.&lt;/p&gt;

&lt;h4 id=&#34;the-netflix-prize&#34;&gt;The Netflix Prize&lt;/h4&gt;

&lt;p&gt;Background: Given (movieID, userID, userRating, timestamp), improve predictions by 10%.
For a long time, the improvment was stuck around 7%. The key was when &lt;em&gt;timestamp&lt;/em&gt; got
incorporated into the predictive models. Essentially, they had to adjust for each humans&amp;rsquo;
bias. Some examples of said bias adjustments:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Alice effect: Alice rates lower than avg.&lt;/li&gt;
&lt;li&gt;Inception effect: Certain movies always get rated above avg&lt;/li&gt;
&lt;li&gt;Overall mean rating of a movie; high ratings encourage high ratings&lt;/li&gt;
&lt;li&gt;# of people who&amp;rsquo;ve rated a movie&lt;/li&gt;
&lt;li&gt;# of days since user&amp;rsquo;s first rating&lt;/li&gt;
&lt;li&gt;# of days since movie&amp;rsquo;s first rating&lt;/li&gt;
&lt;li&gt;Mood, time of day, day of week&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;friday-afternoon&#34;&gt;Friday - Afternoon&lt;/h1&gt;

&lt;h2 id=&#34;deploying-serving-models&#34;&gt;Deploying &amp;amp; Serving models&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://tensorflow.github.io/serving/&#34;&gt;TensorFlow Serving&lt;/a&gt; provides a means of deploying
and running predictions on the models.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Dstream&lt;/code&gt; is the RDD of Spark Streaming; the mini-batch that&amp;rsquo;s just come in.
Holden Karrau - High Performance Spark talks about using RDDs in place of DataFrames&lt;/p&gt;

&lt;p&gt;What he&amp;rsquo;d use in a Production-ready system today:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Workflow: Airflow. Describe DAGs of tasks in Python.&lt;/li&gt;
&lt;li&gt;Extract-Transform-Load (ETL): PySpark. More Python than Java/Scala datasci people running around.&lt;/li&gt;
&lt;li&gt;Serving layer: Redis. Rock-solid, very fast.&lt;/li&gt;
&lt;li&gt;Data Stitching: nifi (maybe). Collects info from many many sources and convert into a single format.&lt;/li&gt;
&lt;li&gt;Storage: Elasticsearch to begin, and maybe forever. Scales well, good APIs; use it until you can&amp;rsquo;t.

&lt;ul&gt;
&lt;li&gt;Spark-Elasticsearch connector is very advanced; takes advantage of data locality&lt;/li&gt;
&lt;li&gt;Starting to tout itself as a GraphDB?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Monitoring: Ganglia.&lt;/li&gt;
&lt;li&gt;Logging: ELK fo&amp;rsquo; sho&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Queueing: Kafka, if you have the ops team to support it. Otherwise, check out AWS Kinesis.&lt;/li&gt;
&lt;li&gt;Streaming

&lt;ul&gt;
&lt;li&gt;Kafka Streams or Flink. May want to &amp;ldquo;write to the Apache Beam API&amp;rdquo; - came out of Google.&lt;/li&gt;
&lt;li&gt;Storm is proven, but not getting a lot of new development. Twitter has written an API-compatible replacement
called Heron.&lt;/li&gt;
&lt;li&gt;Flink has a first-class processing for Complex Data Processing - whatever that
means.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Machine Learning: Depends on your language preference. If Python, then TensorFlow, sklearn, nltk.&lt;/li&gt;
&lt;li&gt;IDL: Spark is &lt;em&gt;all about&lt;/em&gt; Parquet. In-memory version is Apache Arrow.

&lt;ul&gt;
&lt;li&gt;SparkSQL is the most mature. Edit: A later speaker from Terabyte remarked on
SparkSQL being the most &lt;em&gt;immature&lt;/em&gt;, when compared to Apache Hive, Impala, Drill.&lt;/li&gt;
&lt;li&gt;Any project Michael Armbrust is on is going to be developed well&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;good-cool-ideas&#34;&gt;Good &amp;amp; Cool ideas&lt;/h2&gt;

&lt;p&gt;Spark&amp;rsquo;s &lt;code&gt;new StandardScaler=(withMean=True, withStd=False)&lt;/code&gt; lets you toggle how you feature scale.
PCA wants mean normalization, without std dev, but linear regression will want both.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;TextRank&lt;/code&gt;: Finds the sentence that best summarizes the corpus.&lt;/p&gt;

&lt;p&gt;Cluster density can be measured by WSSSE (Within Set Sum of Squared Errors); lower is better.&lt;/p&gt;

&lt;p&gt;Speaker mentioned anecdotally that 2 or 3 friends in the last 6 months had ditched
Cassandra clusters in favor of ElasticSearch.&lt;/p&gt;

&lt;p&gt;You don&amp;rsquo;t want to be doing read-heavy analytics on your write-heavy Cassandra
cluster; either have two separate clusters and setup replication from W=&amp;gt;R, or
read the SSTable files off the disk.&lt;/p&gt;

&lt;p&gt;StitchFix - Fill out preferences for clothes, then a stylist selects five items to ship you. Anything you send back you
fill out why you don&amp;rsquo;t like it. NLP interpets the written response, neural nets learn the style, and they&amp;rsquo;ll even ship
feedback to the designers saying what they&amp;rsquo;ve learned.&lt;/p&gt;

&lt;p&gt;Interesting thing to note: Often mentioned new technologies as a &amp;ldquo;recruiting point&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;skflow: A scikit-learn API compatible replacment with TensorFlow as a backend.&lt;/p&gt;

&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;

&lt;p&gt;&amp;ldquo;Jupyter notebooks for developers&amp;rdquo; &amp;amp; &amp;ldquo;Matplotlib for Developers&amp;rdquo; from O&amp;rsquo;Reilly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Descent: Ascending</title>
      <link>http://jad-b.github.io/post/Gradient%20Descent%20Ascending/</link>
      <pubDate>Fri, 13 May 2016 07:20:33 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Gradient%20Descent%20Ascending/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Gradient descent keeps ascending my cost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Funny story&lt;/em&gt;, the issue wasn&amp;rsquo;t with the gradient descent implementation at all, but rather
the cost function(least squares). I was calculating my error by subtracting my
predicted values from the actual values(y), &lt;code&gt;$error = predicted - y$&lt;/code&gt;, instead of the other
way around, &lt;code&gt;$ error = y - predicted$&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This washed out when it came to calculating cost, since you end up squaring the error:&lt;/p&gt;

&lt;div&gt;$$
    J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{i}) - y^{i})^{2}
$$&lt;/div&gt;

&lt;p&gt;but it &lt;em&gt;doesn&amp;rsquo;t&lt;/em&gt; get squared when you calculate the gradient:&lt;/p&gt;

&lt;div&gt;$$
    \theta_{j} := \alpha\frac{1}{m}\sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
$$&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Testing Timeouts In Go With Channel Selects</title>
      <link>http://jad-b.github.io/post/Testing%20Timeouts%20in%20Go/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://jad-b.github.io/post/Testing%20Timeouts%20in%20Go/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
    &amp;quot;net&amp;quot;
    &amp;quot;testing&amp;quot;
    &amp;quot;time&amp;quot;
)

func TestAddrResolve(t *testing.T) {
    network, addr := &amp;quot;ip4&amp;quot;, &amp;quot;127.0.0.125:44151&amp;quot;
    addrChan := make(chan error)

    // Attempt to resolve IP addr
    go func(ch chan error) {
        _, err := net.DialTimeout(network, addr, 1 * time.Second)
        addrChan &amp;lt;- err
    }(addrChan)

    // Now, see who returns a msg first
    select {
    case e := &amp;lt;-addrChan:
        if e == nil {
            t.Fatalf(&amp;quot;%s://%s should fail to resolve&amp;quot;, network, addr)
        } else if testing.Verbose() { // Success!
            t.Logf(&amp;quot;Call to %s://%s timed out.\nError\n\t%s&amp;quot;, network, addr, e)
        }
    case &amp;lt;-time.After(1 * time.Second):
        t.Fatal(&amp;quot;Address resolution failed to timeout in one second.&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Testing Distributed Systems</title>
      <link>http://jad-b.github.io/post/Testing%20Distributed%20Systems/</link>
      <pubDate>Tue, 19 Apr 2016 14:52:38 -0400</pubDate>
      
      <guid>http://jad-b.github.io/post/Testing%20Distributed%20Systems/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;TL;DR: Takeaways&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Always, always, always handle errors appropriately. No &lt;code&gt;pass&lt;/code&gt;, no &lt;code&gt;/* TODO
*/&lt;/code&gt;. &lt;em&gt;Something&lt;/em&gt; in the chain needs to verify it&amp;rsquo;s handled.&lt;/li&gt;
&lt;li&gt;Using 3 nodes lets you reproduce 98% of error cases in distributed systems.&lt;/li&gt;
&lt;li&gt;77% of catastrophic failures can be reproduced through unit tests&lt;/li&gt;
&lt;li&gt;Log aggressively, and on both sides of events (message passing).&lt;/li&gt;
&lt;li&gt;The big 5 error-ing events:

&lt;ol&gt;
&lt;li&gt;Startup&lt;/li&gt;
&lt;li&gt;Writes from client&lt;/li&gt;
&lt;li&gt;Node down/unreachable&lt;/li&gt;
&lt;li&gt;Configuration change&lt;/li&gt;
&lt;li&gt;Node join&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a work project involving multiple moving pieces begins the move from
proof-of-concept to preparing for production traffic, the various
pieces are beginning to knit into a whole. In particular, a client-driven event
requires that a list of registered services receive an update. Simple enough,
but failure in this system would result in bad API traffic routing, or worse,
all APIs becoming externally unavailable. Undesirable!&lt;/p&gt;

&lt;p&gt;My previous experiences involved nothing more distributed than your basic
web-server=&amp;gt;DB setup, so I took this as an opportunity to learn from other&amp;rsquo;s
mistakes. Searching around turns up the following advice:&lt;/p&gt;

&lt;h3 id=&#34;simple-testing-can-prevent-most-critical-failures&#34;&gt;Simple Testing Can Prevent Most Critical Failures&lt;/h3&gt;

&lt;p&gt;A whitepaper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:whitepaper&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:whitepaper&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; out of the University of Toronto with some incredible
statistics on avoiding the worst-of-the-bad: catastrophic failures&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:catfail&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:catfail&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. They
attribute 92% of CFs to bad error handling, with a further breakdown of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;35% due to

&lt;ul&gt;
&lt;li&gt;Catching but not doing anything about the error&lt;/li&gt;
&lt;li&gt;Aborting on an overly-general error (java&amp;rsquo;s &lt;code&gt;Throwable&lt;/code&gt;, Python&amp;rsquo;s
&lt;code&gt;except:&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;A TODO/FIXME in place, but no handling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And 23% on aborting on a non-fatal error (failed to delete a temporary file)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;77%&lt;/strong&gt; of these failures they could reproduce using only unit tests. Admittedly,
this is their example unit test:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public void testLogRollAfterSplitStart {
    // Create HBase cluster with 1 master and 2 Region Servers
    startMiniCluster();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;which may stretch your imagining of unit tests. I believe what they were
getting at is that the problems are testable within the scope of a single
function&amp;rsquo;s setup/run/cleanup scope. Also, when your definition of unit test is
&amp;ldquo;code that I wrote&amp;rdquo;, and the code that you wrote was HBase, that&amp;rsquo;s quite the
scope.&lt;/p&gt;

&lt;p&gt;Oh, and how about this: &lt;strong&gt;98% of problems could be recreated using no more than
3 nodes&lt;/strong&gt;. Your 120 node Cassandra cluster&amp;rsquo;s dying? Odds are, you only need
three players to recreate it locally.&lt;/p&gt;

&lt;p&gt;An interesting point of difference the author&amp;rsquo;s noted between distributed and
non-distributed systems was that distributed systems tend to have much better
logging. As such, 84% of the studied failures had their triggering events
logged. They logged so much that the author&amp;rsquo;s recommended more advanced log
analysis techniques than a simple &lt;code&gt;grep ERROR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And to wrap this up: Starting up was the most dangerous time for a process, as
summarized under &amp;ldquo;Lessons Learned&amp;rdquo;. More important is to take that list and mix
it up - 90% of the failures could be categorized as a permutation of only three
key events. Just two events interacting accounted for 50% of CFs.&lt;/p&gt;

&lt;h4 id=&#34;further-reading&#34;&gt;Further reading&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;ConfErr - tests configuration errors within a realistic range&lt;/li&gt;
&lt;li&gt;MODIST - Model checking for distributed system&lt;/li&gt;
&lt;li&gt;FATE and DESTINI - Framework for cloud recovery testing&lt;/li&gt;
&lt;li&gt;This looks interesting: KLEE - a code-coverage generator for C programs.
  Can&amp;rsquo;t find any examples for Python though, which would be my use-case.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:whitepaper&#34;&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&#34;&gt;https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:whitepaper&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:catfail&#34;&gt;Failure of the system for a majority to all users.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:catfail&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Expert On Feeling Good</title>
      <link>http://jad-b.github.io/quote/An%20Expert%20On%20Feeling%20Good/</link>
      <pubDate>Thu, 14 Apr 2016 10:18:07 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/An%20Expert%20On%20Feeling%20Good/</guid>
      <description>&lt;p&gt;Their advice on seducing women was simply: &amp;ldquo;Become an expert in how to feel
good.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Born Artists</title>
      <link>http://jad-b.github.io/quote/Born%20Artists/</link>
      <pubDate>Thu, 14 Apr 2016 10:17:18 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Born%20Artists/</guid>
      <description>&lt;p&gt;All children are born artists. The problem is to remain an artist as they
grow up.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Civilization Advances</title>
      <link>http://jad-b.github.io/quote/Civilization%20Advances/</link>
      <pubDate>Tue, 12 Apr 2016 10:28:35 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Civilization%20Advances/</guid>
      <description>&lt;p&gt;Civilization advances by extending the number of important operations that we
can do without thinking about them&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Abstractions</title>
      <link>http://jad-b.github.io/quote/Abstractions/</link>
      <pubDate>Mon, 11 Apr 2016 08:16:04 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Abstractions/</guid>
      <description>&lt;p&gt;Abstractions that seek to simplify but actually complicate the system are
called magic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concurrency vs. Parallelism</title>
      <link>http://jad-b.github.io/quote/Concurrency%20vs.%20Parallelism/</link>
      <pubDate>Sat, 09 Apr 2016 09:25:06 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Concurrency%20vs.%20Parallelism/</guid>
      <description>&lt;p&gt;In programming, concurrency is the composition of independently executing
processes, while parallelism is the simultaneous execution of (possibly
related) computations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shame</title>
      <link>http://jad-b.github.io/quote/Shame/</link>
      <pubDate>Thu, 07 Apr 2016 06:50:35 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Shame/</guid>
      <description>&lt;p&gt;&amp;hellip;And, it turned out to be &amp;ldquo;shame&amp;rdquo;&amp;hellip;.no one wants to talk about it, and the
less you talk about it the more you have it.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I can tell you it boils down to: &amp;hellip;the thing that keeps us away
from love and connection, is our fear that we&amp;rsquo;re not worthy of love and connection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naturally Iterative</title>
      <link>http://jad-b.github.io/quote/Naturally%20Iterative/</link>
      <pubDate>Thu, 07 Apr 2016 06:48:04 -0400</pubDate>
      
      <guid>http://jad-b.github.io/quote/Naturally%20Iterative/</guid>
      <description>&lt;p&gt;Software development is a naturally iterative process that thrives on the
establishment of effective feedback loops, and we deceive ourselves if we
perceive it any other way.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>